// Code generated by "golang-cloudformation". DO NOT EDIT.

package aws_bedrock

import (
	"github.com/ibrt/golang-cloudformation/cfz"
)

const (
	// AWS_Bedrock_Flow_PromptModelInferenceConfiguration__Type is the CloudFormation type for AWS::Bedrock::Flow.PromptModelInferenceConfiguration.
	AWS_Bedrock_Flow_PromptModelInferenceConfiguration__Type = "AWS::Bedrock::Flow.PromptModelInferenceConfiguration"
)

var (
	// AWS_Bedrock_Flow_PromptModelInferenceConfiguration__PropertiesMap reports all the CloudFormation properties for AWS::Bedrock::Flow.PromptModelInferenceConfiguration.
	AWS_Bedrock_Flow_PromptModelInferenceConfiguration__PropertiesMap = struct {
		MaxTokens     string
		StopSequences string
		Temperature   string
		TopP          string
	}{
		MaxTokens:     "MaxTokens",
		StopSequences: "StopSequences",
		Temperature:   "Temperature",
		TopP:          "TopP",
	}

	// AWS_Bedrock_Flow_PromptModelInferenceConfiguration__PropertiesSlice reports all the CloudFormation properties for AWS::Bedrock::Flow.PromptModelInferenceConfiguration.
	AWS_Bedrock_Flow_PromptModelInferenceConfiguration__PropertiesSlice = []string{
		AWS_Bedrock_Flow_PromptModelInferenceConfiguration__PropertiesMap.MaxTokens,
		AWS_Bedrock_Flow_PromptModelInferenceConfiguration__PropertiesMap.StopSequences,
		AWS_Bedrock_Flow_PromptModelInferenceConfiguration__PropertiesMap.Temperature,
		AWS_Bedrock_Flow_PromptModelInferenceConfiguration__PropertiesMap.TopP,
	}
)

// AWS_Bedrock_Flow_PromptModelInferenceConfiguration is a binding for AWS::Bedrock::Flow.PromptModelInferenceConfiguration.
// See: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-bedrock-flow-promptmodelinferenceconfiguration.html
type AWS_Bedrock_Flow_PromptModelInferenceConfiguration struct {
	// MaxTokens is a property.
	// See: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-bedrock-flow-promptmodelinferenceconfiguration.html#cfn-bedrock-flow-promptmodelinferenceconfiguration-maxtokens
	MaxTokens cfz.Expression[float64] `json:"MaxTokens,omitempty"`

	// StopSequences is a property.
	// See: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-bedrock-flow-promptmodelinferenceconfiguration.html#cfn-bedrock-flow-promptmodelinferenceconfiguration-stopsequences
	StopSequences cfz.ExpressionSlice[string] `json:"StopSequences,omitempty"`

	// Temperature is a property.
	// See: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-bedrock-flow-promptmodelinferenceconfiguration.html#cfn-bedrock-flow-promptmodelinferenceconfiguration-temperature
	Temperature cfz.Expression[float64] `json:"Temperature,omitempty"`

	// TopP is a property.
	// See: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-bedrock-flow-promptmodelinferenceconfiguration.html#cfn-bedrock-flow-promptmodelinferenceconfiguration-topp
	TopP cfz.Expression[float64] `json:"TopP,omitempty"`
}

// New__AWS_Bedrock_Flow_PromptModelInferenceConfiguration initializes a new AWS_Bedrock_Flow_PromptModelInferenceConfiguration.
func New__AWS_Bedrock_Flow_PromptModelInferenceConfiguration() AWS_Bedrock_Flow_PromptModelInferenceConfiguration {
	return AWS_Bedrock_Flow_PromptModelInferenceConfiguration{}
}

// GetType returns the CloudFormation type.
func (AWS_Bedrock_Flow_PromptModelInferenceConfiguration) GetType() string {
	return AWS_Bedrock_Flow_PromptModelInferenceConfiguration__Type
}

// Set__MaxTokens updates property "MaxTokens".
func (t AWS_Bedrock_Flow_PromptModelInferenceConfiguration) Set__MaxTokens(v cfz.Expression[float64]) AWS_Bedrock_Flow_PromptModelInferenceConfiguration {
	t.MaxTokens = v
	return t
}

// SetV__MaxTokens updates property "MaxTokens".
func (t AWS_Bedrock_Flow_PromptModelInferenceConfiguration) SetV__MaxTokens(v float64) AWS_Bedrock_Flow_PromptModelInferenceConfiguration {
	t.MaxTokens = cfz.V(v)
	return t
}

// Set__StopSequences updates property "StopSequences".
func (t AWS_Bedrock_Flow_PromptModelInferenceConfiguration) Set__StopSequences(v cfz.ExpressionSlice[string]) AWS_Bedrock_Flow_PromptModelInferenceConfiguration {
	t.StopSequences = v
	return t
}

// SetS__StopSequences updates property "StopSequences".
func (t AWS_Bedrock_Flow_PromptModelInferenceConfiguration) SetS__StopSequences(v ...cfz.Expression[string]) AWS_Bedrock_Flow_PromptModelInferenceConfiguration {
	t.StopSequences = cfz.S(v...)
	return t
}

// SetSV__StopSequences updates property "StopSequences".
func (t AWS_Bedrock_Flow_PromptModelInferenceConfiguration) SetSV__StopSequences(v ...string) AWS_Bedrock_Flow_PromptModelInferenceConfiguration {
	t.StopSequences = cfz.SV(v...)
	return t
}

// Set__Temperature updates property "Temperature".
func (t AWS_Bedrock_Flow_PromptModelInferenceConfiguration) Set__Temperature(v cfz.Expression[float64]) AWS_Bedrock_Flow_PromptModelInferenceConfiguration {
	t.Temperature = v
	return t
}

// SetV__Temperature updates property "Temperature".
func (t AWS_Bedrock_Flow_PromptModelInferenceConfiguration) SetV__Temperature(v float64) AWS_Bedrock_Flow_PromptModelInferenceConfiguration {
	t.Temperature = cfz.V(v)
	return t
}

// Set__TopP updates property "TopP".
func (t AWS_Bedrock_Flow_PromptModelInferenceConfiguration) Set__TopP(v cfz.Expression[float64]) AWS_Bedrock_Flow_PromptModelInferenceConfiguration {
	t.TopP = v
	return t
}

// SetV__TopP updates property "TopP".
func (t AWS_Bedrock_Flow_PromptModelInferenceConfiguration) SetV__TopP(v float64) AWS_Bedrock_Flow_PromptModelInferenceConfiguration {
	t.TopP = cfz.V(v)
	return t
}
